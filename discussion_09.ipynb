{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U3SrcQkGz8e"
      },
      "source": [
        "# CDSDS 542 Deep Learning for Data Science - Discussion 8: Transformer\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/DL4DS/fa2026_discussions/blob/main/discussion_8.ipynb)\n",
        "\n",
        "**Module**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KE1GsvlvGsnM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import math\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split as tts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki0W8ddsd-9G"
      },
      "source": [
        "Main translator structure before transformer:\n",
        "![image](https://eugeneyan.com/assets/encoder-decoder.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE5UGCYFe-hX"
      },
      "source": [
        "## Demo 1: Sequence Reversal with Transformer\n",
        "\n",
        "Goal:\n",
        "- Input: sequence of integers [1, 2, 3, 4]\n",
        "- Target: reversed sequence [4, 3, 2, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeG-dCEUJAhY"
      },
      "source": [
        "![img](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kf871smtQKXAf3dSYOlVPA.png)\n",
        "\n",
        "Two main parts to a transformer:\n",
        "\n",
        "- Encoder:  The encoder takes in the input sentence and produces a fixed-size vector representation of it\n",
        "- Decoder: The decoder takes the fixed-size vector representation and produces the output sentence. The decoder uses both self-attention and cross-attention, where the attention mechanism is applied to the output of the encoder and the input of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NWqjPPUKtdvs"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "\n",
        "# data = [\n",
        "#     ([1, 2, 3, 4], [4, 3, 2, 1]),\n",
        "#     ([5, 6, 7, 8], [8, 7, 6, 5]),\n",
        "#     ([7, 8, 9, 10], [10, 9, 8, 7]),\n",
        "#     ([2, 3, 4, 5], [5, 4, 3, 2])\n",
        "# ]\n",
        "\n",
        "data = []\n",
        "for _ in range(500):\n",
        "    length = random.randint(3, 10)\n",
        "    seq = list(range(1, length+1))\n",
        "    data.append((seq, seq[::-1]))\n",
        "\n",
        "for _ in range(500):\n",
        "    start = random.randint(1, 100)\n",
        "    seq = list(range(start, start+5))\n",
        "    data.append((seq, seq[::-1]))\n",
        "\n",
        "\n",
        "pad = 0\n",
        "vocab_size = 500\n",
        "max_len = max(len(x[0]) for x in data)\n",
        "\n",
        "def pad_seq(seq):\n",
        "    return seq + [pad] * (max_len - len(seq))\n",
        "\n",
        "inputs = torch.tensor([pad_seq(x[0]) for x in data])\n",
        "targets = torch.tensor([pad_seq(x[1]) for x in data])\n",
        "\n",
        "# print(\"Input:\\n\", inputs)\n",
        "# print(\"Target:\\n\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5K_GiVGvMVF"
      },
      "source": [
        "### 1. `Embedding`:\n",
        "\n",
        "`nn.Embedding(vocab_size, d_model, padding_idx)`\n",
        "\n",
        "This step is for word vector mapping. Input a sequence of tokens and output the vector representation of each token, converting discrete ids into computable vectors\n",
        "\n",
        "- `vocab_size`: Vocabulary list size, +1 to allocate an additional position for the padding token.\n",
        "- `d_model`: The Embedding dimension (how many dimensional vectors are used to represent each token), determines the number of columns in the Embedding matrix.\n",
        "- `padding_idx`: Specify which token ID is used for padding, the embedding at this position has a gradient of 0 during training.\n",
        "\n",
        "Why Padding?\n",
        "- Neural networks usually require batch processing to improve efficiency. However, different sequences have different lengths and cannot directly form a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0IgLFtXvZik",
        "outputId": "014acee2-be93-4120-9b99-d912c62ba169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded input:  tensor([[ 0.3649, -0.0367,  0.4655, -0.7512],\n",
            "        [-0.0338, -2.1567, -0.6836, -0.9363],\n",
            "        [-1.3292,  0.5497, -0.6537, -0.4422],\n",
            "        [-0.4421,  0.0802,  0.1117,  0.4469]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 20\n",
        "d_model = 4\n",
        "\n",
        "word_embeddings = nn.Embedding(vocab_size+1, d_model, padding_idx=0)\n",
        "embedded = word_embeddings(torch.tensor([1, 2, 3, 4]))\n",
        "print(\"Embedded input: \", embedded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVNXVTJOuFFV"
      },
      "source": [
        "### 2. `PositionalEncoding` class:\n",
        "- Adds positional information to the input embeddings by summing them with positional encodings of the same dimensions. The positional encodings are calculated using sine and cosine functions of different frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-CAMXQLdAfO"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe8M3PjZ3AMn",
        "outputId": "74a32326-6107-46b0-9313-9d469f526d72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
              "       [ 0.84147096,  0.54030234,  0.00999983,  0.99995   ],\n",
              "       [ 0.9092974 , -0.41614684,  0.01999867,  0.9998    ],\n",
              "       [ 0.14112   , -0.9899925 ,  0.0299955 ,  0.99955004]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_encoding = PositionalEncoding(d_model=4)\n",
        "pe_matrix = pos_encoding.pe[0, : 4, :].detach().numpy()\n",
        "pe_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtc8xuaFC3AS"
      },
      "source": [
        "So, for this situation:\n",
        "\n",
        "- embedded (4×4) : Random word vectors representing \"what word it is\"\n",
        "- pe_matrix (4×4) : Fixed position encoding, representing \"at which position\"\n",
        "- output = embedded + pe_matrix: Knowing both \"what word\" and \"where\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDPZFaqbEL2l"
      },
      "source": [
        "### 3. `DotProductAttention` class:\n",
        "\n",
        "Computes the attention scores by performing a dot product between queries (Q) and keys (K), scales these scores by the square root of the query dimension, applies a mask based on valid lengths to ignore padding, applies softmax to get probabilities, and finally computes the weighted sum of values (V).\n",
        "\n",
        "- A “query” is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
        "- The “key” is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match with the query.\n",
        "- The “value” in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.\n",
        "\n",
        "How to understand Q, K, V:\n",
        "\n",
        "| Symbol | Mean     | Analogy      |\n",
        "| -- | ------ | ------- |\n",
        "| Q  | What am I looking for?| The person asking the question.|\n",
        "| K  | What do I have to offer? | Information index tags.|\n",
        "| V  | What information should I share if chosen? | The content of my answer itself |\n",
        "\n",
        "- From basic linear-algebra, we know that matrices are nothing but the linear transformations or rules that operate on vectors and change their properties like rotate them by a certain angle, reflect them about some axis, etc\n",
        "- These `trainable matrices` for query, keys and values do something similar – stretch, shear, or elongate the manifolds such that the `similarity of the alike words increases whereas for dissimilar words it decreases.`\n",
        "\n",
        "- In a nutshell, transforming vectors with matrices can increase/decrease the similarity score and hence the attention weights between two vectors.\n",
        "    - This is what K, Q and V do to the input embedding vectors. They are trainable meaning during the course of training, their weights will be optimized to change the manifold. This will increase/decrease the similarity between tokens on the basis of the loss function optimization during training.\n",
        "\n",
        "![img](https://storrs.io/content/images/2021/08/Screen-Shot-2021-08-07-at-7.51.37-AM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap7IJh99d6uT"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q,K,V shapes: (batch, heads, seq, d_k)\n",
        "        scores = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))  # QK^T / sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(0).unsqueeze(0)  # (1,1,L,L)\n",
        "\n",
        "            mask = mask.to(dtype=torch.bool, device=scores.device)\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "        attn = scores.softmax(dim=-1)\n",
        "        return attn @ V, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADxoi4lKKCb2"
      },
      "outputs": [],
      "source": [
        "# calculate with an example\n",
        "batch, heads, seq_len, d_k = 1, 1, 4, 8\n",
        "torch.manual_seed(42)\n",
        "Q = torch.randn(batch, heads, seq_len, d_k) * 0.5\n",
        "K = torch.randn(batch, heads, seq_len, d_k) * 0.5\n",
        "V = torch.randn(batch, heads, seq_len, d_k) * 0.5\n",
        "\n",
        "# using Attention\n",
        "attention = ScaledDotProductAttention()\n",
        "output, attn_weights = attention(Q, K, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3PxbPjEbgRI"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUKiDf-0bzYI"
      },
      "outputs": [],
      "source": [
        "batch, heads, seq_len, d_k = 1, 1, 4, 8\n",
        "torch.manual_seed(42)\n",
        "Q = torch.randn(batch, heads, seq_len, d_k) * 0.5\n",
        "K = torch.randn(batch, heads, seq_len, d_k) * 0.5\n",
        "V = torch.randn(batch, heads, seq_len, d_k) * 0.5\n",
        "\n",
        "# TODO: calculate the attention block by hand\n",
        "K_T = K.transpose(-2, -1)\n",
        "scores = __fill__\n",
        "print(f\"scores: [1,1,4,8] @ [1,1,8,4] = {scores.shape}\")\n",
        "attn = __fill__\n",
        "attn_np = attn[0, 0].detach().numpy() # attention weight matrix\n",
        "print(f\"sum of rows = {attn_np.sum(axis=1)}\")\n",
        "output =  __fill__  # [1,1,4,8]\n",
        "print(f\"attn @ V: [1,1,4,4] @ [1,1,4,8] = {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-l3hpYBJiKg",
        "outputId": "e01c1365-74f5-423b-fef3-bd6d678ada0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scores: [1,1,4,8] @ [1,1,8,4] = torch.Size([1, 1, 4, 4])\n",
            "sum of rows = [1. 1. 1. 1.]\n",
            "attn @ V: [1,1,4,4] @ [1,1,4,8] = torch.Size([1, 1, 4, 8])\n"
          ]
        }
      ],
      "source": [
        "# expected output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5fd7CmxWYLo",
        "outputId": "5e553bec-4726-4607-ed98-1e182c3940ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: check if your answer is correct\n",
        "torch.allclose(torch.tensor(attn_np), attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Df5hFgXvQ5"
      },
      "source": [
        "#### Attention Mask\n",
        "\n",
        "The `mask` parameter controls which positions can attend to which positions. When `mask[i,j] == 0`, the attention mechanism prevents position `i` from attending to position `j`.\n",
        "\n",
        "**Mechanism:**\n",
        "```python\n",
        "scores = scores.masked_fill(mask == 0, -1e9)\n",
        "```\n",
        "Positions where `mask == 0` are filled with `-1e9` (negative infinity). After `softmax()`, these positions get attention weights ≈ 0, effectively \"hiding\" them from the model.\n",
        "\n",
        "**Two Common Types:**\n",
        "\n",
        "1. **Padding Mask** - Prevents attention to padding tokens (`<PAD>`)\n",
        "   - Shape: `[batch, 1, 1, seq_len]` or `[batch, 1, seq_len, seq_len]`\n",
        "   - Example: `[1, 1, 1, 0]` masks the last position\n",
        "   - Purpose: Ignore meaningless padding positions in variable-length sequences\n",
        "\n",
        "2. **Causal Mask** - Prevents attention to future positions (autoregressive models)\n",
        "   - Shape: Lower triangular matrix\n",
        "      ```\n",
        "        [[1, 0, 0, 0],    # token 0 can only see token 0\n",
        "          [1, 1, 0, 0],    # token 1 can see tokens 0-1\n",
        "          [1, 1, 1, 0],    # token 2 can see tokens 0-2\n",
        "          [1, 1, 1, 1]]    # token 3 can see tokens 0-3\n",
        "      ```\n",
        "   - Purpose: Used in GPT-style models to prevent \"looking ahead\" during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2HC8J5iEUhi"
      },
      "source": [
        "### 4. `MultiHeadAttention` class:\n",
        "\n",
        "Transforms the input queries, keys, and values using the initialized weight matrices, splits these transformations into multiple heads, applies dot-product attention in each head, concatenates the heads' outputs, and finally linearly transforms the concatenated output.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb1zJ_t-d6rs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Q,K,V trans\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x_q, x_k, x_v, mask=None):\n",
        "        B, Lq, D = x_q.size()\n",
        "        B, Lk, D = x_k.size()\n",
        "        B, Lv, D = x_v.size()\n",
        "\n",
        "        # (B, L, D) → (B, num_heads, L, d_k)\n",
        "        Q = self.W_Q(x_q).contiguous().view(B, Lq, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_K(x_k).contiguous().view(B, Lk, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_V(x_v).contiguous().view(B, Lv, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        out, attn = self.attention(Q, K, V, mask)\n",
        "        out = out.transpose(1,2).contiguous().view(B, Lq, D)\n",
        "        return self.out(out), attn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ONFy1QqVxSn"
      },
      "source": [
        "**Scaled Dot-Product Attention**\n",
        "\n",
        "Self-attention computes a weighted sum of values based on the similarity between queries and keys. The mechanism consists of three steps:\n",
        "\n",
        "**Step 1: Compute Attention Scores**\n",
        "```\n",
        "scores = Q @ K^T / √d_k\n",
        "```\n",
        "- `Q @ K^T`: Compute pairwise similarity between all tokens → shape `[seq_len, seq_len]`\n",
        "- `√d_k`: Scale factor to prevent gradients from vanishing when `d_k` is large\n",
        "- `scores[i,j]`: Raw attention score representing how much token `i` should attend to token `j`\n",
        "\n",
        "**Step 2: Normalize to Attention Weights**\n",
        "```\n",
        "attn = softmax(scores, dim=-1)\n",
        "```\n",
        "- Apply `softmax` row-wise to convert scores into probability distributions\n",
        "- `attn[i,j]`: Normalized attention weight indicating the importance of token `j` when processing token `i`\n",
        "- Each row sums to 1: `Σⱼ attn[i,j] = 1`\n",
        "\n",
        "**Step 3: Compute Weighted Output**\n",
        "```\n",
        "output = attn @ V\n",
        "```\n",
        "- Aggregate value vectors using attention weights\n",
        "- `output[i] = Σⱼ attn[i,j] × V[j]`: Each output is a weighted combination of all value vectors\n",
        "- The model \"gathers\" information from relevant positions based on learned attention patterns\n",
        "\n",
        "**Summary:** `Attention(Q, K, V) = softmax(QK^T / √d_k) × V`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xKM7NMEZYZ3"
      },
      "source": [
        "### 5. `Encoder` Block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXpwanvrd6pF"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model))\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        x = self.norm2(x + self.ff(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5gDzAdzkgf0"
      },
      "source": [
        "The encoder block consists of two sub-layers, each followed by residual connection and layer normalization.\n",
        "\n",
        "**Architecture:**\n",
        "```python\n",
        "x → Multi-Head Attention → Add & Norm → Feed-Forward → Add & Norm → output\n",
        "    ↓_________________↑                   ↓___________↑\n",
        "       residual connection               residual connection\n",
        "```\n",
        "\n",
        "**Components:**\n",
        "\n",
        "1. **Multi-Head Attention**\n",
        "   ```python\n",
        "   attn_out, _ = self.attn(x, x, x)  # Self-attention: Q=K=V=x\n",
        "   ```\n",
        "   - Enables the model to attend to different positions and capture various relationships\n",
        "   - Output shape: `[batch, seq_len, d_model]`\n",
        "\n",
        "2. **First Add & Norm**\n",
        "   ```python\n",
        "   x = self.norm1(x + attn_out)\n",
        "   ```\n",
        "   - `x + attn_out`: Residual connection preserves original information and facilitates gradient flow\n",
        "   - `norm1`: Layer normalization stabilizes training by normalizing across the feature dimension\n",
        "\n",
        "3. **Feed-Forward Network (FFN)**\n",
        "   ```python\n",
        "   self.ff = nn.Sequential(\n",
        "       nn.Linear(d_model, d_ff),  # Expand: d_model → d_ff (typically d_ff = 4 × d_model)\n",
        "       nn.ReLU(),                  # Non-linearity\n",
        "       nn.Linear(d_ff, d_model)   # Project back: d_ff → d_model\n",
        "   )\n",
        "   ```\n",
        "   - Processes each position independently (position-wise)\n",
        "   - Adds non-linear transformation capacity to the model\n",
        "\n",
        "4. **Second Add & Norm**\n",
        "   ```python\n",
        "   x = self.norm2(x + self.ff(x))\n",
        "   ```\n",
        "   - Another residual connection followed by layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osi_FqjDZcV8"
      },
      "source": [
        "### 6. `Decoder` Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6ObcV5Ud6mv"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model))\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_out):\n",
        "        seq_len = x.size(1)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), 1).bool().to(x.device)\n",
        "\n",
        "        out, _ = self.self_attn(x, x, x, mask=mask)\n",
        "        x = self.norm1(x + out)\n",
        "\n",
        "        out, _ = self.cross_attn(x, enc_out, enc_out)\n",
        "        x = self.norm2(x + out)\n",
        "\n",
        "        x = self.norm3(x + self.ff(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soV0y4VFk9J6"
      },
      "source": [
        "The decoder block extends the encoder block with an additional cross-attention layer, enabling it to attend to the encoder's output. It consists of three sub-layers, each with residual connection and layer normalization.\n",
        "\n",
        "**Architecture:**\n",
        "```python\n",
        "x → Masked Self-Attention → Add & Norm → Cross-Attention → Add & Norm → FFN → Add & Norm → output\n",
        "    ↓___________________↑                 ↓___________↑              ↓_______↑\n",
        "      residual connection                residual connection      residual connection\n",
        "```\n",
        "\n",
        "**Components:**\n",
        "\n",
        "1. **Masked Self-Attention**\n",
        "   ```python\n",
        "   mask = torch.triu(torch.ones(seq_len, seq_len), 1).bool()  # Upper triangular\n",
        "   out, _ = self.self_attn(x, x, x, mask=mask)\n",
        "   x = self.norm1(x + out)\n",
        "   ```\n",
        "2. **Cross-Attention** (Encoder-Decoder Attention)\n",
        "   ```python\n",
        "   out, _ = self.cross_attn(x, enc_out, enc_out)  # Q=decoder, K=V=encoder\n",
        "   x = self.norm2(x + out)\n",
        "   ```\n",
        "   - **Query (Q)**: From decoder (current decoding state)\n",
        "   - **Key (K) & Value (V)**: From encoder output (source sequence information)\n",
        "   - Allows decoder to attend to relevant parts of the input sequence\n",
        "\n",
        "3. **Feed-Forward Network**\n",
        "   ```python\n",
        "   x = self.norm3(x + self.ff(x))\n",
        "   ```\n",
        "   - Same structure as encoder: `Linear(d_model → d_ff) → ReLU → Linear(d_ff → d_model)`\n",
        "   - Position-wise transformation applied independently to each token\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f3ls4-SZn1o"
      },
      "source": [
        "### 7. Define `Transformer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOptooTqd6ka"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=32, num_heads=1, d_ff=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size+1, d_model, padding_idx=0)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, vocab_size+1)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Encoder\n",
        "        src = self.pos(self.embed(src))\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src)\n",
        "        enc_out = src\n",
        "\n",
        "        # Decoder\n",
        "        tgt = self.pos(self.embed(tgt))\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, enc_out)\n",
        "\n",
        "        return self.fc(tgt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRQ0F8jJlSJs"
      },
      "source": [
        "The full Transformer architecture combines the encoder and decoder stacks with embedding and output layers.\n",
        "\n",
        "**Architecture Overview:**\n",
        "```python\n",
        "Input (token IDs)\n",
        "  ↓\n",
        "Embedding + Positional Encoding\n",
        "  ↓\n",
        "Encoder Stack (N layers)\n",
        "  ↓\n",
        "Decoder Stack (N layers) ← receives encoder output\n",
        "  ↓\n",
        "Linear Projection (to vocabulary)\n",
        "  ↓\n",
        "Output (logits over vocabulary)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYcgmoRXd6hU",
        "outputId": "8ffa590a-2bed-41e7-a1f9-90d0b6bb7d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss=6.1965\n",
            "Epoch 50, Loss=0.0718\n",
            "Epoch 100, Loss=0.0055\n",
            "Epoch 150, Loss=0.0023\n",
            "Epoch 200, Loss=0.0014\n",
            "Epoch 250, Loss=0.0010\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 501\n",
        "model = Transformer(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(300):\n",
        "    opt.zero_grad()\n",
        "    tgt_in = torch.cat([torch.full((targets.size(0), 1), 1, device=targets.device), targets[:, :-1]], dim=1)\n",
        "    out = model(inputs, tgt_in)\n",
        "    loss = criterion(out.view(-1, out.size(-1)), targets.view(-1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss={loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ampAUqgAPo",
        "outputId": "ffee8e10-4215-4a39-8e4e-4630c7b18c4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[28, 27, 26, 25]])\n"
          ]
        }
      ],
      "source": [
        "def generate(model, src, max_len):\n",
        "    model.eval()\n",
        "    src = src.unsqueeze(0)\n",
        "\n",
        "    enc = model.pos(model.embed(src))\n",
        "    for layer in model.encoder_layers:\n",
        "        enc = layer(enc)\n",
        "\n",
        "    BOS = 1\n",
        "    tgt = torch.tensor([[BOS]], device=src.device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        dec = model.pos(model.embed(tgt))\n",
        "        for layer in model.decoder_layers:\n",
        "            dec = layer(dec, enc)\n",
        "        logits = model.fc(dec)\n",
        "        next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
        "        tgt = torch.cat([tgt, next_tok], dim=1)\n",
        "\n",
        "    return tgt[:, 1:]\n",
        "\n",
        "test = torch.tensor([25,26,27,28])\n",
        "print(generate(model, test, max_len=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUr9_HZVd7vI"
      },
      "source": [
        "## Demo 2: String Reversal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOqUiQbdd6b3",
        "outputId": "5237e38e-e628-4571-eeb5-fc31eaf2f66c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            " tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12],\n",
            "        [13, 14, 15, 16]])\n",
            "targets:\n",
            " tensor([[ 4,  3,  2,  1],\n",
            "        [ 8,  7,  6,  5],\n",
            "        [12, 11, 10,  9],\n",
            "        [16, 15, 14, 13]])\n"
          ]
        }
      ],
      "source": [
        "sent_pairs = [\n",
        "    (\"I love deep learning\", \"learning deep love I\"),\n",
        "    (\"Transformers are so powerful\", \"powerful so are Transformers\"),\n",
        "    (\"Attention is a magic\", \"magic a is Attention\"),\n",
        "    (\"Neural networks learn patterns\", \"patterns learn networks Neural\"),\n",
        "]\n",
        "\n",
        "# build vocab\n",
        "vocab = {\"<pad>\":0}\n",
        "for s, t in sent_pairs:\n",
        "    for w in (s + \" \" + t).split():\n",
        "        if w not in vocab:\n",
        "            vocab[w] = len(vocab)\n",
        "\n",
        "inv_vocab = {i:w for w,i in vocab.items()}\n",
        "pad = vocab[\"<pad>\"]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "max_len = max(len(s.split()) for s,_ in sent_pairs)\n",
        "\n",
        "def pad_seq(seq):\n",
        "    idxs = [vocab[w] for w in seq.split()]\n",
        "    return idxs + [pad]*(max_len - len(idxs))\n",
        "\n",
        "inputs = torch.tensor([pad_seq(s) for s,_ in sent_pairs])      # Encoder input\n",
        "targets = torch.tensor([pad_seq(t) for _,t in sent_pairs])     # Decoder target\n",
        "\n",
        "print(\"inputs:\\n\", inputs)\n",
        "print(\"targets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduHBZ1FeFsH"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPX-KC3Ffkje"
      },
      "source": [
        "### Exercise 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNfYgxbXeCGH",
        "outputId": "7491c688-8707-4d5a-9cdd-b3f2627f7af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss=2.8952\n",
            "Epoch 200, Loss=0.0015\n",
            "Epoch 400, Loss=0.0005\n",
            "Epoch 600, Loss=0.0003\n",
            "Epoch 800, Loss=0.0002\n"
          ]
        }
      ],
      "source": [
        "# TODO: define the training\n",
        "model = __fill__\n",
        "criterion = __fill__\n",
        "opt = __fill__\n",
        "\n",
        "for epoch in range(300):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # construct decoder input：<bos> we use the location of pad index 0 as initial（or create new <bos>）\n",
        "    bos = torch.full((targets.size(0),1), pad)  # <bos>\n",
        "    tgt_in = torch.cat([bos, targets[:, :-1]], dim=1)\n",
        "\n",
        "    out = model(inputs, tgt_in)   # transformer forward\n",
        "    loss = criterion(out.reshape(-1, vocab_size + 1), targets.reshape(-1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss={loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVI-xrwTe3R8"
      },
      "source": [
        "**Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7z79YXCdgcN"
      },
      "outputs": [],
      "source": [
        "# TODO: test if it's correct\n",
        "def decode(model, sentence):\n",
        "    model.eval()\n",
        "    src = torch.tensor([pad_seq(sentence)])\n",
        "    tgt = torch.full((1,1), pad)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        out = model(src, tgt)\n",
        "        next_word = out[:,-1].argmax(-1, keepdim=True)\n",
        "        tgt = torch.cat([tgt, next_word], dim=1)\n",
        "\n",
        "    pred_idxs = tgt[0,1:].tolist()\n",
        "    pred_words = [inv_vocab[i] for i in pred_idxs if i != pad]\n",
        "    return \" \".join(pred_words)\n",
        "\n",
        "for s,_ in sent_pairs:\n",
        "    print(f\"{s}  →  {decode(model, s)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qcLE2nBe2WL",
        "outputId": "cc5daea4-9451-45cc-ef58-f95cf5ea2cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I love deep learning  →  learning deep love I\n",
            "Transformers are so powerful  →  powerful so are Transformers\n",
            "Attention is a magic  →  magic a is Attention\n",
            "Neural networks learn patterns  →  patterns learn networks Neural\n"
          ]
        }
      ],
      "source": [
        "# Expected output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
